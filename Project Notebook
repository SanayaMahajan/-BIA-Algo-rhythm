{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Latest FINAL NOTEBOOK] dap - rf - mf = 200, 1:1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYsqzEMT3GZe",
        "colab_type": "text"
      },
      "source": [
        "# 1. Loading Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MINLBcg1lgU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "from sklearn.feature_extraction import text\n",
        "\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "#from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmlyYfaN3Oa1",
        "colab_type": "text"
      },
      "source": [
        "# 2. Loading Data and Basic Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD7ZKwpi3nPe",
        "colab_type": "text"
      },
      "source": [
        "The dataset we used can be found here: https://www.kaggle.com/neisse/scrapped-lyrics-from-6-genres?select=lyrics-data.csv\n",
        "- Only English songs were used from this dataset\n",
        "- Gender of the artist was manually added to this dataset (by creating a list of unique distinct artist names, and adding the corresponding gender in)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRlGIq2Ylzfg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f1b3cc8a-c42b-4383-a008-eb20189743ef"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ed9VsKqDLaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('gdrive/My Drive/final_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK-GHjAQl4Fb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1b53887a-6cbc-497c-f549-755e6be97132"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>ALink</th>\n",
              "      <th>SName</th>\n",
              "      <th>Lyric</th>\n",
              "      <th>Gender</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>/10000-maniacs/</td>\n",
              "      <td>More Than This</td>\n",
              "      <td>I could feel at the time. There was no way of ...</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>/10000-maniacs/</td>\n",
              "      <td>Because The Night</td>\n",
              "      <td>Take me now, baby, here as I am. Hold me close...</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>/10000-maniacs/</td>\n",
              "      <td>These Are Days</td>\n",
              "      <td>These are. These are days you'll remember. Nev...</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>/10000-maniacs/</td>\n",
              "      <td>A Campfire Song</td>\n",
              "      <td>A lie to say, \"O my mountain has coal veins an...</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>/10000-maniacs/</td>\n",
              "      <td>Everyday Is Like Sunday</td>\n",
              "      <td>Trudging slowly over wet sand. Back to the ben...</td>\n",
              "      <td>M</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ... Gender\n",
              "0           0  ...      M\n",
              "1           1  ...      M\n",
              "2           2  ...      M\n",
              "3           3  ...      M\n",
              "4           4  ...      M\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n569mJvvD_Yk",
        "colab_type": "text"
      },
      "source": [
        "We further dropped songs of mixed gender bands (coded as \"-\") as well as those for which artists couldn't be found on credible sources (coded as \"?\"). Ultimately, we removed those items and were left with ~100K songs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm7WABatmHWF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "39a54fda-2a4b-4479-cc9e-4ce06025b4fd"
      },
      "source": [
        "df = df.drop(['Unnamed: 0'], axis = 1)\n",
        "df = df[~df['Gender'].isin(['-', '?'])]\n",
        "df['Gender'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "M    81578\n",
              "F    24078\n",
              "Name: Gender, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOkcaSNf5Jgm",
        "colab_type": "text"
      },
      "source": [
        "As can be seen from the above value counts, our dataset was highly imbalanced. Thus, we resorted to random undersampling of the male songs to make the ratio of male:female songs 1:1 and then train our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6UUgKwc5978",
        "colab_type": "text"
      },
      "source": [
        "As a first step, we tranformed all lyrics into lowercase and removed punctuations as well as special characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrcciE_7mN03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Lyric'] = df['Lyric'].str.lower()\n",
        "df['Lyric'] = df['Lyric'].apply(lambda x: re.sub('[^a-z0-9]', ' ', x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6oUqRIL6eNb",
        "colab_type": "text"
      },
      "source": [
        "Using Label encoder, we then encoded the Gender column in the form of 0 (F - female) and 1 (M - male) for classification purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jEBvpE5mkdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "le = LabelEncoder()\n",
        "df['Gender_code'] = le.fit_transform(df['Gender']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS985Q8gDr9t",
        "colab_type": "text"
      },
      "source": [
        "We finally dropped 57500 male songs to balance out the ratio of male to female songs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghfsVUQfFy3X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "014265e5-15b8-4ced-b6e4-c4d8b236c508"
      },
      "source": [
        "df = df.drop(df.query('Gender_code == 1').sample(n=57500).index)\n",
        "df['Gender'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "M    24078\n",
              "F    24078\n",
              "Name: Gender, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffLs0_bA7CmS",
        "colab_type": "text"
      },
      "source": [
        "From our previous investigations of this problem with a smaller dataset (done as part of DAP), we had found Random Forest to be a suitable fit. Thus, we decided to go ahead with that as our classfier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjSHOrCBHV1J",
        "colab_type": "text"
      },
      "source": [
        "To start off, we first did a train-test split, followed by feature extraction, hyperparameter tuning and finally, model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcP6CrQ2E4eg",
        "colab_type": "text"
      },
      "source": [
        "# 3. Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa9Iki_FGPl4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df['Lyric']\n",
        "y = df['Gender_code']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELaU7kXTGTcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ac3JBIGCVoW",
        "colab_type": "text"
      },
      "source": [
        "# 4. Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REYqETt68CwS",
        "colab_type": "text"
      },
      "source": [
        "We decided to use Tfidf Vectorizer to get the term frequency — inverse document frequency (tf-idf in short) of each word as normalized input into the models. We decided to extract a maximum of 200 features with highest tf-idf scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJFny7GG7fXn",
        "colab_type": "text"
      },
      "source": [
        "We first created a function to get Part Of Speech (POS) tag for each word (such as noun, verb, adjective, etc) to improve the lemmatization quality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tV1YrZuYn1K8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_wordnet_pos(treebank_tag):\n",
        "\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return nltk.corpus.wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return nltk.corpus.wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return nltk.corpus.wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return nltk.corpus.wordnet.ADV\n",
        "    else:\n",
        "        return nltk.corpus.wordnet.NOUN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93ydUl5T90Q9",
        "colab_type": "text"
      },
      "source": [
        "We also defined a tokenizing and lemmatizing function that will be later used in our application."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl15eDvvn4yo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lem = nltk.WordNetLemmatizer()\n",
        "    \n",
        "def tokenize_and_lemmatize(para):\n",
        "    tokens = [word for word in nltk.word_tokenize(para)]\n",
        "    \n",
        "    #remove non alphabetical tokens\n",
        "    filtered_tokens = []\n",
        "    for token in tokens:\n",
        "        if token.isalpha():\n",
        "            filtered_tokens.append(token)\n",
        "   \n",
        "    lemmatized_tokens = []        \n",
        "   \n",
        "    for (item,pos_tag) in nltk.pos_tag(filtered_tokens):\n",
        "        lemmatized_token = lem.lemmatize(item, get_wordnet_pos(pos_tag))\n",
        "        lemmatized_tokens.append(lemmatized_token)\n",
        "                             \n",
        "    #return filtered_tokens\n",
        "    return lemmatized_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKuLpdc192H1",
        "colab_type": "text"
      },
      "source": [
        "We further downloaded stopwords dictionary from NLTK to remove stopwords in English such as pronouns and modal verbs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gURRghF8n75Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b3c2c211-3338-4fe0-f800-945ca4208de3"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1F3OUmnqn9bV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "single_letters = set(string.ascii_lowercase)\n",
        "nltk_stop_words = set(nltk.corpus.stopwords.words('english')).union(single_letters)\n",
        "stopwords = set(text.ENGLISH_STOP_WORDS.union(nltk_stop_words))\n",
        "\n",
        "error = ['far', 'make', 'need', 'sha', 'wo']\n",
        "stopwords = set(stopwords.union(error))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1yZZlRUCZ1M",
        "colab_type": "text"
      },
      "source": [
        "We then obtained the tfidf scores of top 200 words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voWNes27GVPR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "4900bf5d-3f76-477d-f309-b5f554442114"
      },
      "source": [
        "tv = text.TfidfVectorizer(tokenizer = tokenize_and_lemmatize, stop_words=stopwords, ngram_range = (1,1), max_features = 200)\n",
        "tv.fit(X_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=1.0, max_features=200,\n",
              "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True,\n",
              "                stop_words={'a', 'about', 'above', 'across', 'after',\n",
              "                            'afterwards', 'again', 'against', 'ain', 'all',\n",
              "                            'almost', 'alone', 'along', 'already', 'also',\n",
              "                            'although', 'always', 'am', 'among', 'amongst',\n",
              "                            'amoungst', 'amount', 'an', 'and', 'another', 'any',\n",
              "                            'anyhow', 'anyone', 'anything', 'anyway', ...},\n",
              "                strip_accents=None, sublinear_tf=False,\n",
              "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=<function tokenize_and_lemmatize at 0x7f7ca6827ea0>,\n",
              "                use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBS2afeuGZGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_count =  tv.transform(X_train)\n",
        "X_test_count =  tv.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfBiUE79IN-a",
        "colab_type": "text"
      },
      "source": [
        "# 5. Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A74tO8ww4gKq",
        "colab_type": "text"
      },
      "source": [
        "Given the big size of the dataset, we decided to use a subset of it for hyperparameter tuning. We used 2000 male songs and 2000 female songs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIMxnFIgKGRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tuning_data = pd.concat([X_train, y_train], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EhoOzsAKWWj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0d00773d-45bd-4fd5-9ada-ca900a1782bd"
      },
      "source": [
        "tuning_data[\"Gender_code\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    16855\n",
              "0    16854\n",
              "Name: Gender_code, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKVMEUE1mmxO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e289ee8e-dcfd-4d5c-b297-7ead8ca15550"
      },
      "source": [
        "tuning_data = tuning_data.drop(tuning_data.query('Gender_code == 1').sample(n=14855).index)\n",
        "tuning_data = tuning_data.drop(tuning_data.query('Gender_code == 0').sample(n=14854).index)\n",
        "tuning_data['Gender_code'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    2000\n",
              "0    2000\n",
              "Name: Gender_code, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfAbKEDToAFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_tune = tuning_data['Lyric']\n",
        "y_tune = tuning_data['Gender_code']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F90IBiAoF_L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "d9411ebd-13ae-4499-f133-22cfd03d495c"
      },
      "source": [
        "tv_tune = text.TfidfVectorizer(tokenizer = tokenize_and_lemmatize, stop_words=stopwords, ngram_range = (1,1), max_features = 200)\n",
        "tv_tune.fit(X_tune)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=1.0, max_features=200,\n",
              "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True,\n",
              "                stop_words={'a', 'about', 'above', 'across', 'after',\n",
              "                            'afterwards', 'again', 'against', 'ain', 'all',\n",
              "                            'almost', 'alone', 'along', 'already', 'also',\n",
              "                            'although', 'always', 'am', 'among', 'amongst',\n",
              "                            'amoungst', 'amount', 'an', 'and', 'another', 'any',\n",
              "                            'anyhow', 'anyone', 'anything', 'anyway', ...},\n",
              "                strip_accents=None, sublinear_tf=False,\n",
              "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=<function tokenize_and_lemmatize at 0x7f7ca6827ea0>,\n",
              "                use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCA02zcwoMUv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_tune_count =  tv_tune.transform(X_tune)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwoYWISEEf_Z",
        "colab_type": "text"
      },
      "source": [
        "## 5.1 Randomized Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fIHukOG-yQ4",
        "colab_type": "text"
      },
      "source": [
        "After extracting the tfidf scores of the top 200 features (or top 200 words), we defined a grid of potential hyperparameters for the random forest classifier. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLYfQNLeoWAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rf = RandomForestRegressor(random_state = 0)\n",
        "\n",
        "# Number of trees in random forest\n",
        "n_estimators = [100, 200, 300, 500]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [10, 20, 50, 100]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10, 20]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 5, 10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mst1_r8IpIBG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTSRHypE_JFc",
        "colab_type": "text"
      },
      "source": [
        "Using the grid defined above, a randomized 3-fold cross-validation search over 100 combinations of hyperparameters was performed. This was done to narrow down the range of potential hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Hwdj1KGpSD3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "4355f901-cbd3-45e6-f28a-f0de6ca800f6"
      },
      "source": [
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "rf = RandomForestRegressor()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=0, n_jobs = -1)# Fit the random search model\n",
        "rf_random.fit(X_tune_count, y_tune)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  9.6min\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed: 34.2min\n",
            "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 57.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3, error_score=nan,\n",
              "                   estimator=RandomForestRegressor(bootstrap=True,\n",
              "                                                   ccp_alpha=0.0,\n",
              "                                                   criterion='mse',\n",
              "                                                   max_depth=None,\n",
              "                                                   max_features='auto',\n",
              "                                                   max_leaf_nodes=None,\n",
              "                                                   max_samples=None,\n",
              "                                                   min_impurity_decrease=0.0,\n",
              "                                                   min_impurity_split=None,\n",
              "                                                   min_samples_leaf=1,\n",
              "                                                   min_samples_split=2,\n",
              "                                                   min_weight_fraction_leaf=0.0,\n",
              "                                                   n_estimators=100,\n",
              "                                                   n_jobs=None, oob_score=False,\n",
              "                                                   random_state=None, verbose=0,\n",
              "                                                   warm_start=False),\n",
              "                   iid='deprecated', n_iter=100, n_jobs=-1,\n",
              "                   param_distributions={'max_depth': [10, 20, 50, 100, None],\n",
              "                                        'max_features': ['auto', 'sqrt'],\n",
              "                                        'min_samples_leaf': [1, 2, 5, 10],\n",
              "                                        'min_samples_split': [2, 5, 10, 20],\n",
              "                                        'n_estimators': [100, 200, 300, 500]},\n",
              "                   pre_dispatch='2*n_jobs', random_state=0, refit=True,\n",
              "                   return_train_score=False, scoring=None, verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5gx4UImpdeB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "73c54671-a8f9-4889-e4c0-9a6965aaa754"
      },
      "source": [
        "rf_random.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'max_depth': None,\n",
              " 'max_features': 'sqrt',\n",
              " 'min_samples_leaf': 2,\n",
              " 'min_samples_split': 2,\n",
              " 'n_estimators': 500}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guaIvoZ-Eond",
        "colab_type": "text"
      },
      "source": [
        "## 5.2 Grid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ech_u0edAxgF",
        "colab_type": "text"
      },
      "source": [
        "Based on the results from Randomized Search above, we narrowed down the range of our potential hyperparameter grid as follows and then performed the more exhaustive Grid Search, which iterated over all possible hyperparameter combinations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6i45oDt_GyK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the parameter grid based on the results of random search \n",
        "param_grid = {\n",
        "    'max_depth': [50, 80, 100, None], \n",
        "    'max_features': ['sqrt'],\n",
        "    'min_samples_leaf': [1, 2, 5], \n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'n_estimators': [100, 200, 500] \n",
        "}\n",
        "# Create a based model\n",
        "rf = RandomForestRegressor()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgHry7Pw_0Y7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 5, n_jobs = -1, verbose = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPnghsgf_2ZB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "903505bd-7c5f-4111-f7ba-b592edc0c55a"
      },
      "source": [
        "grid_search.fit(X_tune_count, y_tune)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  2.6min\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed: 10.7min\n",
            "[Parallel(n_jobs=-1)]: Done 361 tasks      | elapsed: 25.2min\n",
            "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed: 37.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=RandomForestRegressor(bootstrap=True, ccp_alpha=0.0,\n",
              "                                             criterion='mse', max_depth=None,\n",
              "                                             max_features='auto',\n",
              "                                             max_leaf_nodes=None,\n",
              "                                             max_samples=None,\n",
              "                                             min_impurity_decrease=0.0,\n",
              "                                             min_impurity_split=None,\n",
              "                                             min_samples_leaf=1,\n",
              "                                             min_samples_split=2,\n",
              "                                             min_weight_fraction_leaf=0.0,\n",
              "                                             n_estimators=100, n_jobs=None,\n",
              "                                             oob_score=False, random_state=None,\n",
              "                                             verbose=0, warm_start=False),\n",
              "             iid='deprecated', n_jobs=-1,\n",
              "             param_grid={'max_depth': [50, 80, 100, None],\n",
              "                         'max_features': ['sqrt'],\n",
              "                         'min_samples_leaf': [1, 2, 5],\n",
              "                         'min_samples_split': [2, 5, 10],\n",
              "                         'n_estimators': [100, 200, 500]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atxb-IXm_6uK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "44b844f8-258a-4dfe-a871-fb660edf0d6b"
      },
      "source": [
        "grid_search.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'max_depth': 80,\n",
              " 'max_features': 'sqrt',\n",
              " 'min_samples_leaf': 1,\n",
              " 'min_samples_split': 2,\n",
              " 'n_estimators': 500}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUMdap8tBTUO",
        "colab_type": "text"
      },
      "source": [
        "From Grid Search, we got the result that our Random Forest model should have hyperparameters as shown above. We then used these hyperparameters to train the model using the entire training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjsgw-qsCti-",
        "colab_type": "text"
      },
      "source": [
        "# 6. Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aokPdC0MCzYN",
        "colab_type": "text"
      },
      "source": [
        "Just for sake of comparison, we first trained a model with default random forest classifier hyperparameters and obtained the accuracy as well as F1 score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2rBHdDDGgwh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80c48af6-9c6c-4b4c-9252-b570525dcd52"
      },
      "source": [
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train_count, y_train)\n",
        "y_pred_clf = clf.predict(X_test_count)\n",
        "accuracy_clf = metrics.accuracy_score(y_test, y_pred_clf)\n",
        "f1_clf = metrics.f1_score(y_test, y_pred_clf)\n",
        "print(accuracy_clf, f1_clf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7380078909116079 0.7394148020654044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXCOujNEDWiW",
        "colab_type": "text"
      },
      "source": [
        "We then trained the model with our shortlisted hyperparameters in the Hyperparameter Tuning section. Again, accuracy as well as F1 score were obtained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmiQSDxyGlKg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "434a4811-8642-4a8a-fac5-c35a700868a1"
      },
      "source": [
        "clf_1 = RandomForestClassifier(max_depth=80, max_features='sqrt', min_samples_leaf=1, min_samples_split=2, n_estimators = 500)\n",
        "clf_1.fit(X_train_count, y_train)\n",
        "y_pred_clf_1 = clf_1.predict(X_test_count)\n",
        "accuracy_clf_1 = metrics.accuracy_score(y_test, y_pred_clf_1)\n",
        "f1_clf_1 = metrics.f1_score(y_test, y_pred_clf_1)\n",
        "print(accuracy_clf_1, f1_clf_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7467294247940749 0.752385463896596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjy-ODUJDbnM",
        "colab_type": "text"
      },
      "source": [
        "As can be seen, the accuracy and F1 score were slightly higher than for the model with default hyperparameter values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnsP2ZArF2Kc",
        "colab_type": "text"
      },
      "source": [
        "With a decent accuracy and F1 score, we went ahead to deploy the model with shortlisted hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvCpK3N7FLdK",
        "colab_type": "text"
      },
      "source": [
        "# 7. Deployment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMG7Th2D1UDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7AYSSBnzvdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename1 = 'finalized_model.pkl'\n",
        "pickle.dump(clf_1, open(filename1, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LdE5n1e3Oo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename2 = 'tv.pkl'\n",
        "pickle.dump(tv, open(filename2, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6Ur2yBC3kUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#filename3 = 'X_train_count.pkl'\n",
        "#pickle.dump(X_train_count, open(filename3, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoYe92cc3pKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#filename4 = 'X_test_count.pkl'\n",
        "#pickle.dump(X_test_count, open(filename4, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QghYOahI6Bvc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "5333560c-4326-43fb-f51f-49e8fb3d7630"
      },
      "source": [
        "#filename5 = 'tfidf.pkl'\n",
        "#pickle.dump(tv.fit(X_train), open(filename5, 'wb'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C_vesR34B74",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "23854d49-6d9a-4970-c6ef-5c68015515ea"
      },
      "source": [
        "files.download('finalized_model.pkl')\n",
        "files.download('tv.pkl')\n",
        "#files.download('X_train_count.pkl')\n",
        "#files.download('X_test_count.pkl')\n",
        "#files.download('tfidf.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_1c0e46f5-245a-4792-a949-4ca995168c82\", \"finalized_model.pkl\", 423433826)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_3561a12d-d551-43bf-a0f6-6042774e00c2\", \"tv.pkl\", 984994)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_7bdeeee9-ffd2-40c3-b45f-13a69ce9cab0\", \"X_train_count.pkl\", 10247955)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_fdfd62d1-7bff-4a43-b30f-54cae2cd0946\", \"X_test_count.pkl\", 4407783)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_e63c30ad-f5d7-417a-ba5e-f76f40120295\", \"tfidf.pkl\", 984994)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}
